{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "97ebcfa3-b5fd-408c-a6b9-d0fd92827331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as torch_optim\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5341f52-2d28-400f-87df-bb4a71f58ec0",
   "metadata": {},
   "source": [
    "## Genre Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "a8c25fd6-8073-4b81-9059-75904aee491a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenreClassifier(nn.Module):\n",
    "    def __init__(self, inputs):\n",
    "        super(GenreClassifier, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(inputs.shape[1] * inputs.shape[2], 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        \n",
    "        self.optimizer = torch_optim.Adam(model.parameters(), lr=0.001)\n",
    "        self.loss      = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(Tensor(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc38ffa2-b38c-41a7-8940-62f1a28a7d2e",
   "metadata": {},
   "source": [
    "### Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "52dd8e13-e7a1-46e4-8a60-bba278ec1f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 22050\n",
    "DURATION = 30\n",
    "SAMPLES_PER_TRACK = SAMPLE_RATE * DURATION\n",
    "N_FFT = 2048\n",
    "HOP_LENGTH = 512\n",
    "N_MFCC = 13\n",
    "MFCC_OUT_PATH = \"mfcc_batches.json\"\n",
    "\n",
    "def segmented_batch_save_mfcc(\n",
    "    dataset_path,\n",
    "    n_mfcc=N_MFCC,\n",
    "    n_fft=N_FFT,\n",
    "    hop_length=HOP_LENGTH,\n",
    "    n_segments=5\n",
    "):\n",
    "    extractings = {\"mapping\": [], \"labels\": [], \"MFCCs\": []}\n",
    "    N_SAMPLES_PER_SEGMENT = int(SAMPLES_PER_TRACK / n_segments)\n",
    "    EXPECTED_MFCC_PER_SEGMENT = np.ceil(N_SAMPLES_PER_SEGMENT / hop_length)\n",
    "    \n",
    "    for genre_idx, (dirpath, _dirnames, filenames) in enumerate(os.walk(dataset_path)):\n",
    "        if dirpath is not dataset_path:\n",
    "            dirpath_components = dirpath.split(\"/\")\n",
    "            semantic_label = dirpath_components[-1]\n",
    "            extractings[\"mapping\"].append(semantic_label)\n",
    "            for f in filenames:\n",
    "                filepath = os.path.join(dirpath, f)\n",
    "                signal, sr = librosa.load(filepath, sr=SAMPLE_RATE)\n",
    "                for s in range(n_segments):\n",
    "                    start_sample = N_SAMPLES_PER_SEGMENT * s\n",
    "                    finish_sample = start_sample + N_SAMPLES_PER_SEGMENT\n",
    "                    mfcc = librosa.feature.mfcc(\n",
    "                        y=signal[start_sample:finish_sample],\n",
    "                        sr=sr,\n",
    "                        n_fft=n_fft,\n",
    "                        n_mfcc=n_mfcc,\n",
    "                        hop_length=hop_length\n",
    "                    )\n",
    "                    MFCC = mfcc.T\n",
    "                    if len(MFCC) == EXPECTED_MFCC_PER_SEGMENT:\n",
    "                        extractings[\"MFCCs\"].append(MFCC.tolist())\n",
    "                        extractings[\"labels\"].append(genre_idx - 1)\n",
    "    with open(MFCC_OUT_PATH, \"w\") as out:\n",
    "        json.dump(extractings, out, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23d20b8-6d86-484c-b925-b40483e29da3",
   "metadata": {},
   "source": [
    "### Trainer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "1454619f-6a92-4a92-bc37-d460e354f356",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(proc_dataset_path=MFCC_OUT_PATH):\n",
    "    with open(proc_dataset_path, \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "    inputs = np.array(data[\"MFCCs\"])\n",
    "    targets = np.array(data[\"labels\"])\n",
    "    return inputs, targets\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, inputs, targets):\n",
    "        self.model = model\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "    def split_data(self, train_size=0.8, test_size=0.2):\n",
    "        assert(train_size+test_size == 1)\n",
    "        x_train, x_test, y_train, y_test = train_test_split(\n",
    "            self.inputs,\n",
    "            self.targets,\n",
    "            test_size=test_size,\n",
    "        )\n",
    "        \n",
    "        return x_train, x_test, y_train, y_test\n",
    "    def find_latest_ckpt(self, ckpt_dir):\n",
    "        ckpts = []\n",
    "        for filename in os.listdir(ckpt_dir):\n",
    "            if filename.startswith(\"ckpt\"):\n",
    "                ckpts.append(filename)\n",
    "        for ckpt in sorted(ckpts):\n",
    "            ckpt_num = int(re.findall(r\"\\d+\", ckpt)[0])\n",
    "        new_ckpt_num = ckpt_num\n",
    "        return new_ckpt_num\n",
    "    def train(\n",
    "        self,\n",
    "        epochs=10,\n",
    "        batch_size=64,\n",
    "        lr=0.0001,\n",
    "    ):\n",
    "        latest_ckpt_n = self.find_latest_ckpt(\"checkpoint\")\n",
    "        msg = \"Spinning up from ckpt \"\n",
    "        ckpt_path = \"checkpoint/ckpt_\" + str(latest_ckpt_n) + \".pth\"\n",
    "        ckpt_state = torch.load(ckpt_path)\n",
    "        self.model.load_state_dict(ckpt_state)\n",
    "        \n",
    "        x_train, x_test, y_train, y_test = self.split_data(train_size=0.7, test_size=0.3)\n",
    "        for epoch in range(epochs):\n",
    "            for batch in range(len(x_train)//batch_size):\n",
    "                batch_x = x_train[batch * batch_size : (batch+1) * batch_size]\n",
    "                batch_y = y_train[batch * batch_size : (batch + 1) * batch_size]\n",
    "                x = self.model.forward(batch_x)\n",
    "                self.model.optimizer.zero_grad()\n",
    "                loss = self.model.loss(x, torch.tensor(batch_y))\n",
    "                loss.backward()\n",
    "                self.model.optimizer.step()\n",
    "\n",
    "        print(\"Saving model weights...\")\n",
    "        CKPT_DIR = \"checkpoint\"\n",
    "        latest_ckpt_n = self.find_latest_ckpt(CKPT_DIR)\n",
    "        checkpoint = {\n",
    "            \"model_state_dict\": self.model.state_dict(),\n",
    "            \"optimizer_state_dict\": self.model.optimizer.state_dict(),\n",
    "            \"epoch\": epoch\n",
    "        }\n",
    "        ckpt_file_new = \"checkpoint/ckpt_\" + str(latest_ckpt_n+1) + \".pth\"\n",
    "        torch.save(self.model.state_dict(), ckpt_file_new)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889e97d0-81e8-4719-93e4-b17c6f836998",
   "metadata": {},
   "source": [
    "## Perform Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "0f9398ab-c6ba-4ad5-b95b-bbc9c8a18959",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNPROCESSED_DATASET = \"example-train\"\n",
    "segmented_batch_save_mfcc(UNPROCESSED_DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9961fa5f-01a2-49ce-829c-77159118c797",
   "metadata": {},
   "source": [
    "## Perform Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "0a9fb775-8c49-4020-a6a5-a91fc3d00687",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets = load_data(MFCC_OUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "241e9827-ee8a-4bf6-8fbb-318f30d76aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 1 complete.\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 2 complete.\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 3 complete.\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 4 complete.\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 5 complete.\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 6 complete.\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 7 complete.\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 8 complete.\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 9 complete.\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 10 complete.\n",
      "Saving model weights...\n"
     ]
    }
   ],
   "source": [
    "model = GenreClassifier(inputs)\n",
    "trainer = Trainer(model, inputs, targets)\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f66a312-e0df-4912-9dac-0153d43c0323",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c528aa89-6c34-4fb7-a9a6-0f889669adbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
